ARG BASE_IMAGE
FROM ${BASE_IMAGE}

ARG SCRIPT_DIR=/tmp/dockerfile_scripts
RUN mkdir -p ${SCRIPT_DIR}

# Remove the ompi/ucx, etc that is in the base image
# Seems that the torch installed in the NGC image links against this.
# Wonder if that will cause problems? We can have it use our OMPI but it
# also wants libucs, etc, from UCX. The NGC container must be building
# torch from source and enabling torch distributed with mpi backend.
#RUN rm -rf /opt/hpcx /usr/local/mpi

# Put all HPC related tools we build under /container/hpc so we can
# have a shared include, lib, bin, etc to simplify our paths and build steps.
ARG HPC_DIR=/container/hpc
RUN mkdir -p ${HPC_DIR}/bin && \
    mkdir -p ${HPC_DIR}/lib && \
    mkdir -p ${HPC_DIR}/include && \
    mkdir -p ${HPC_DIR}/share && \
    ln -s ${HPC_DIR}/lib ${HPC_DIR}/lib64 && \
    chmod -R go+rX ${HPC_DIR}
ENV LD_LIBRARY_PATH=$HPC_DIR/lib:$LD_LIBRARY_PATH
ENV PATH=$HPC_DIR/bin:$PATH

# Setup some default env variables. This is for the end user as well
# as tools we will build since we put include files under HPC_DIR.
COPY dockerfile_scripts/setup_sh_env.sh ${SCRIPT_DIR}
RUN ${SCRIPT_DIR}/setup_sh_env.sh

# We run this here even though it might be a repeat from the base image
# to make sure we have the required bits for building NCCL, libcxi, etc.
COPY dockerfile_scripts/install_deb_packages.sh ${SCRIPT_DIR}
RUN ${SCRIPT_DIR}/install_deb_packages.sh

# Should we just use /container as the install dir and put
# everything (ie, ucx/ofi/ompi/mpich) under /container/{bin|lib}
# to clean up these arguments?
# Install Cray CXI headers/lib
COPY dockerfile_scripts/cray-libs.sh ${SCRIPT_DIR}
RUN ${SCRIPT_DIR}/cray-libs.sh

# Install all HPC related tools under /container (e.g., mpi, ofi, etc).
ARG MPI_TYPE
COPY dockerfile_scripts/ompi.sh ${SCRIPT_DIR}
RUN ${SCRIPT_DIR}/ompi.sh

# Enable running OMPI as root. Note that we only need this if we use OMPI. 
ENV OMPI_ALLOW_RUN_AS_ROOT 1
ENV OMPI_ALLOW_RUN_AS_ROOT_CONFIRM 1
# Need to override this so we don't try using the OMPI built into the
# base container, which is not built correctly for libfabric
ENV OPAL_PREFIX=${WITH_MPI:+$HPC_DIR}

# If we built MPI, override any MPI in /usr/local/mpi that might
# have been installed by NVIDIA targeting IB.
RUN rm -rf /usr/local/mpi && ln -s /container/hpc /usr/local/mpi

# Set an entrypoint that can scrape up the host libfabric.so and then
# run the user command. This is intended to enable performant execution
# on non-IB systems that have a proprietary libfabric.
COPY dockerfile_scripts/scrape_libs.sh ${SCRIPT_DIR}
RUN mkdir -p /container/bin && \
    cp ${SCRIPT_DIR}/scrape_libs.sh /container/bin
ENTRYPOINT ["/container/bin/scrape_libs.sh"]

RUN rm -r /tmp/*
